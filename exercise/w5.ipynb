{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d64b89b-83dd-4afb-8382-cfe85673d915",
   "metadata": {},
   "source": [
    "# Notebook for Week 5: NLP algorithms and models as concept representation tools\n",
    "\n",
    "Key points: \n",
    "- methodological background and overview\n",
    "- vector semantics and embeddings\n",
    "- Word2Vec, Doc2Vec\n",
    "- semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9375642a-f295-4503-ab14-04e61f943b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, stanza, cld3, logging, random, nltk, torch\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from toolz import partition\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text # https://github.com/tensorflow/tensorflow/issues/38597\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492e404-b567-4f7e-bc94-329f53e62601",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904c85ec-77ff-4a95-91d4-9f1697d0588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a text document\n",
    "doc=open('txt/aarpus-a178676817aa4aee08acc216161a5cf3-3164bffa749c22e91ff36ea0fbd98fb4').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876822aa-201a-448b-a636-c0b016482578",
   "metadata": {},
   "source": [
    "## With [NLTK](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028a6162-c7ff-42f1-914b-9f3740428d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'does',\n",
       " 'earnings',\n",
       " 'inequality',\n",
       " 'affect',\n",
       " 'social',\n",
       " 'security',\n",
       " 'financing',\n",
       " '?',\n",
       " '-']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower case and tokenize\n",
    "from nltk import word_tokenize\n",
    "doc_tokens = word_tokenize(doc.lower())\n",
    "doc_tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18fd1212-3144-477e-b462-933def63333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7518a69-2426-4383-a178-22a15e8391d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earnings',\n",
       " 'inequality',\n",
       " 'affect',\n",
       " 'social',\n",
       " 'security',\n",
       " 'financing',\n",
       " '?',\n",
       " '-',\n",
       " 'aarp',\n",
       " 'insight']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens_clean=[word for word in doc_tokens if word not in stop_words]\n",
    "doc_tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6751687d-a738-41c3-9ceb-005829b4947b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earnings',\n",
       " 'inequality',\n",
       " 'affect',\n",
       " 'social',\n",
       " 'security',\n",
       " 'financing',\n",
       " 'aarp',\n",
       " 'insight',\n",
       " 'issues',\n",
       " 'lality']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "from string import punctuation\n",
    "doc_tokens_clean=[word for word in doc_tokens_clean if word not in punctuation]\n",
    "doc_tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b61ad3c4-3163-4448-9404-eb9aab60ea0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earn',\n",
       " 'inequ',\n",
       " 'affect',\n",
       " 'social',\n",
       " 'secur',\n",
       " 'financ',\n",
       " 'aarp',\n",
       " 'insight',\n",
       " 'issu',\n",
       " 'laliti']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "doc_tokens_clean_stemmed = [porter.stem(word) for word in doc_tokens_clean]\n",
    "doc_tokens_clean_stemmed[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "130a23bd-7dac-46f8-8794-6937f0f6e24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1457, 1347, 1212)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare size of tokens in different sets\n",
    "len(set(doc_tokens)), len(set(doc_tokens_clean)), len(set(doc_tokens_clean_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e75d8c-f372-4619-bbfc-ae6f2858c1fe",
   "metadata": {},
   "source": [
    "## With [Stanza](https://stanfordnlp.github.io/stanza/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647da6be-1ef2-46d0-9238-3e21004637bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 04:47:13 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae00bed1ae541919c1ec8b5e84227ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 04:47:13 WARNING: Can not find mwt: default from official model list. Ignoring it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffc6566403e423b8d077e1d57c22ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/pos/combined.pt:   0%|         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e53e436b53488ab399fd7b0b82515b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/lemma/combined.pt:   0%|       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e730327e330d4c13bc28c1f33f28a02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/pretrain/combined.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 04:47:16 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2023-02-13 04:47:16 INFO: Use device: cpu\n",
      "2023-02-13 04:47:16 INFO: Loading: tokenize\n",
      "2023-02-13 04:47:16 INFO: Loading: pos\n",
      "2023-02-13 04:47:16 INFO: Loading: lemma\n",
      "2023-02-13 04:47:16 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#### Tokenize and remove stopwords according to Universal POS tags using Stanza, English only.\n",
    "# https://universaldependencies.org/u/pos/\n",
    "''' with sentence split, try True later '''\n",
    "nlp_en = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma', \n",
    "                         tokenize_no_ssplit=False, \n",
    "                         use_gpu=True)\n",
    "remove_pos_list=['DET', 'SYM', 'PUNCT', 'PART', 'CCONJ', 'SCONJ', 'AUX', 'X', 'ADP']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682510b-587a-4b9a-8f8c-4dc24916e518",
   "metadata": {},
   "source": [
    "### Process a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75524eb9-720d-416d-8f57-6738f919b940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_doc=nlp_en(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "688be7e3-f928-4b1e-90fe-05b9536ddbe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.763803680981596"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(sent.to_dict()) for sent in out_doc.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7902a18d-7a3b-4ec4-9e94-f67541bda907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['taxable',\n",
       " 'maximum',\n",
       " 'adjust',\n",
       " 'change',\n",
       " 'consumer',\n",
       " 'price',\n",
       " 'index',\n",
       " 'urban',\n",
       " 'wage',\n",
       " 'earner',\n",
       " 'clerical',\n",
       " 'worker']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a sentence\n",
    "[word['lemma'] for word in out_doc.sentences[32].to_dict() if word['upos'] not in remove_pos_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f67e02fe-fbe8-470b-aa8a-edd0e6a41235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate all sentences\n",
    "doc_tokens_clean_stanza=[]\n",
    "for sent in out_doc.sentences:\n",
    "    doc_tokens_clean_stanza+=[word['lemma'] for word in sent.to_dict() if word['upos'] not in remove_pos_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb6bcb86-c236-4376-86cb-57189d21ce07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1213"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(doc_tokens_clean_stanza))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eafe3c0-9228-408f-90a5-df91ec38f3fc",
   "metadata": {},
   "source": [
    "### Process documents in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "844987e5-8ee4-4c1c-84c6-233b693d3eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 19:21:58 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396aeca900b04c72b1930b1bb380edcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 19:21:58 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "WARNING:stanza:Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-02-10 19:21:58 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2023-02-10 19:21:58 INFO: Use device: gpu\n",
      "INFO:stanza:Use device: gpu\n",
      "2023-02-10 19:21:58 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2023-02-10 19:21:58 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2023-02-10 19:21:59 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2023-02-10 19:21:59 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#### Tokenize and remove stopwords according to Universal POS tags using Stanza, English only.\n",
    "# https://universaldependencies.org/u/pos/\n",
    "nlp_en = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma', \n",
    "                         tokenize_no_ssplit=True, \n",
    "                         use_gpu=True)\n",
    "remove_pos_list=['DET', 'SYM', 'PUNCT', 'PART', 'CCONJ', 'SCONJ', 'AUX', 'X', 'ADP']\n",
    "\n",
    "def func_stanza_clean_en(documents):\n",
    "    logging.info('Wrap documents as stanza objects ...')\n",
    "    in_docs = [stanza.Document([], text=d) for d in documents] # Wrap each document as a stanza.Document object\n",
    "    logging.info('NLP-ing ...')\n",
    "    out_docs = nlp_en(in_docs)\n",
    "    # Remove stopwords and not meaningful words using POS tagging.\n",
    "    # https://universaldependencies.org/u/pos/\n",
    "    remove_pos_list=['DET', 'SYM', 'PUNCT', 'PART', 'CCONJ', 'SCONJ', 'AUX', 'X', 'ADP']\n",
    "    logging.info('Cleaning docs using POS tags ...')\n",
    "    txt_tokens=[[t['lemma'] for t in token.to_dict() if t['upos'] not in remove_pos_list] for doc in out_docs for token in doc.sentences]\n",
    "    return txt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2afe15-e560-48e3-b9fc-345f90ef82fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How Does Earnings Inequality Affect Social Security Financing? - AARP Insight on the Issues lality is exacerbating Social problems. Economic inequality past four decades; wages and much more rapidly near the top ‘ibution than at the middle and son, and Smith 2018; Kopczuk, 5 Piketty and Saez 2006; s trend limits Social Security ; Favreault 2009; Morrissey Security exempts from payroll imnings above a certain level ver time, Social Security's : slowly as additional earnings 1g more than Social Se\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=[open('txt/'+file).read() for file in os.listdir('txt/')]\n",
    "docs[-1][0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ae8ed9a7-8ac7-41c9-985b-ec9e72f46829",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens_clean_stanza=func_stanza_clean_en(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80606728-f52f-4467-b422-56007909f488",
   "metadata": {},
   "source": [
    "### NER with Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03ae9c45-c691-4b40-bca4-24e91b45554d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 05:08:35 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3860e1f76701496cb4d755906ff0e6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 05:08:36 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-02-13 05:08:36 INFO: Use device: cpu\n",
      "INFO:stanza:Use device: cpu\n",
      "2023-02-13 05:08:36 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2023-02-13 05:08:36 INFO: Loading: ner\n",
      "INFO:stanza:Loading: ner\n",
      "2023-02-13 05:08:36 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "doc = nlp(docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b59f349-b6c3-4bfe-9ffb-98589cc3767d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: The PI Council\ttype: ORG\n",
      "entity: Maputo\ttype: GPE\n",
      "entity: MEPI Publications\ttype: ORG\n",
      "entity: 51\ttype: CARDINAL\n",
      "entity: SDFs\ttype: ORG\n",
      "entity: the National Heart, Lung and Blood Institute\ttype: ORG\n",
      "entity: MEPI\ttype: ORG\n",
      "entity: the past five years\ttype: DATE\n",
      "entity: CUGH\ttype: ORG\n",
      "entity: UB SoM)\ttype: ORG\n",
      "entity: 50 percent\ttype: PERCENT\n",
      "entity: MEPI\ttype: ORG\n",
      "entity: May 2016\ttype: DATE\n",
      "entity: three\ttype: CARDINAL\n",
      "entity: the United States\ttype: GPE\n",
      "entity: Global Health\ttype: ORG\n",
      "entity: 2\ttype: CARDINAL\n",
      "entity: four\ttype: CARDINAL\n",
      "entity: two\ttype: CARDINAL\n",
      "entity: Schultz JH\ttype: PERSON\n"
     ]
    }
   ],
   "source": [
    "print(*random.sample([f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], 20), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "181de3b0-46a7-4948-b8fc-80fb545f1b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emory',\n",
       " 'the African Union',\n",
       " 'Johns Hopkins University Bloomberg School of Public Health Pilot Award',\n",
       " 'University',\n",
       " 'African Center for Global Health & Social Transformation',\n",
       " 'Ayeni AJ',\n",
       " 'MEPI',\n",
       " 'Duke',\n",
       " 'Emergency Units',\n",
       " 'MMed',\n",
       " 'MDGs',\n",
       " 'RSC',\n",
       " 'NIH Fogarty International Center',\n",
       " 'University of Denver University of KwaZulu Natal',\n",
       " 'an Office of Research Administration',\n",
       " 'the Wellcome Trust',\n",
       " 'University of Botswana',\n",
       " 'MEPI',\n",
       " 'Willems B',\n",
       " 'UB']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample([s.to_dict()['text'] for s in doc.entities if s.type=='ORG'], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91427a0a-a278-44a8-8ef2-ca51ab7bca2d",
   "metadata": {},
   "source": [
    "# Word2Vec examples\n",
    "\n",
    "Source: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "\n",
    "Example studies:\n",
    "\n",
    "- Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review 84 (5): 905–49. https://doi.org/10.1177/0003122419877135.\n",
    "- Jones, Jason J., Mohammad Ruhul Amin, Jessica Kim, and Steven Skiena. 2020. “Stereotypical Gender Associations in Language Have Decreased Over Time.” Sociological Science 7 (January): 1–35. https://doi.org/10.15195/v7.a1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073811dc-9287-4ffe-8c9a-45d4de6a254b",
   "metadata": {},
   "source": [
    "## [Google News](https://code.google.com/archive/p/word2vec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "38794800-799b-4309-8780-2e38a8789140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "082e6e86-fe14-4e66-9822-564b22267b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookup the vector values of a word.\n",
    "wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "daa5ad11-efff-4990-a4fe-e29292c1be91",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'cameroon' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115932/3148784572.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Unfortunately, the model is unable to infer vectors for unfamiliar words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This is one limitation of Word2Vec: if this limitation matters to you, check out the FastText model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cameroon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'cameroon' not present\""
     ]
    }
   ],
   "source": [
    "# Unfortunately, the model is unable to infer vectors for unfamiliar words. \n",
    "# This is one limitation of Word2Vec: if this limitation matters to you, check out the FastText model.\n",
    "wv['cameroon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0271c721-a1de-41c3-a041-f550be344db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "# Calculate word similarity.\n",
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99abb40e-36e6-4118-b869-a2d125a143aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138045430183411),\n",
       " ('queen', 0.6510956287384033),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159993410110474),\n",
       " ('sultan', 0.5864822864532471),\n",
       " ('ruler', 0.5797566771507263),\n",
       " ('princes', 0.5646552443504333),\n",
       " ('Prince_Paras', 0.5432944297790527),\n",
       " ('throne', 0.5422104597091675)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3a920bca-765e-4e02-bb69-ba66e9c63dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chess', 0.6993610858917236),\n",
       " ('grandmasters', 0.6455792784690857),\n",
       " ('grandmaster', 0.6356159448623657),\n",
       " ('blindfold_chess', 0.6172971129417419),\n",
       " ('chess_grandmaster', 0.5920405983924866),\n",
       " ('Kasparov_Karpov', 0.5834527611732483),\n",
       " ('Anatoli_Karpov', 0.5782327055931091),\n",
       " ('backgammon', 0.5762614011764526),\n",
       " ('Korchnoi', 0.5705455541610718),\n",
       " ('Scrabble', 0.5702669024467468)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('chess')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710d9dc-7c1e-42d6-bc53-26ef4b272eb4",
   "metadata": {},
   "source": [
    "## [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "520056b0-4c93-41c9-95d0-21f89b95ab21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-13 04:58:16--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-02-13 04:58:16--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 38s  \n",
      "\n",
      "2023-02-13 05:00:55 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "# Download GloVe from source page: https://nlp.stanford.edu/projects/glove/\n",
    "# These are shell commands\n",
    "''' Make sure you are using `root` or change the folder path accordingly'''\n",
    "!mkdir /root/glove/ && cd /root/glove/ && wget https://nlp.stanford.edu/data/glove.6B.zip && unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d1cf1e-332c-4e9f-a812-aa760c139f41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29027/1106359909.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  _ = glove2word2vec(glove_file, tmp_file)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_file = datapath('/root/glove/glove.6B.50d.txt')\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a985f14-df8d-486e-a1b4-70b63d44a65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.092086,  0.2571  , -0.58693 , -0.37029 ,  1.0828  , -0.55466 ,\n",
       "       -0.78142 ,  0.58696 , -0.58714 ,  0.46318 , -0.11267 ,  0.2606  ,\n",
       "       -0.26928 , -0.072466,  1.247   ,  0.30571 ,  0.56731 ,  0.30509 ,\n",
       "       -0.050312, -0.64443 , -0.54513 ,  0.86429 ,  0.20914 ,  0.56334 ,\n",
       "        1.1228  , -1.0516  , -0.78105 ,  0.29656 ,  0.7261  , -0.61392 ,\n",
       "        2.4225  ,  1.0142  , -0.17753 ,  0.4147  , -0.12966 , -0.47064 ,\n",
       "        0.3807  ,  0.16309 , -0.323   , -0.77899 , -0.42473 , -0.30826 ,\n",
       "       -0.42242 ,  0.055069,  0.38267 ,  0.037415, -0.4302  , -0.39442 ,\n",
       "        0.10511 ,  0.87286 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eee94b7b-5c56-424f-8154-ccb4b1a3e292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'m\", 0.9142324328422546),\n",
       " ('everyone', 0.8976402878761292),\n",
       " ('everybody', 0.8965491056442261),\n",
       " ('really', 0.8839760422706604),\n",
       " ('me', 0.8784631490707397),\n",
       " ('definitely', 0.8762789368629456),\n",
       " ('maybe', 0.8756703734397888),\n",
       " (\"'d\", 0.8718011975288391),\n",
       " ('feel', 0.8707677721977234),\n",
       " ('i', 0.8707453012466431)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d4a5b-733e-4be4-a841-0ad29bd583cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Calculate document similarity between documents/paragraphs/sentences\n",
    "\n",
    "Example study: Ma, Ji, and René Bekkers. 2023. “Consensus Formation in Nonprofit and Philanthropic Studies: Networks, Reputation, and Gender.” Nonprofit and Voluntary Sector Quarterly, January, 08997640221146948. https://doi.org/10.1177/08997640221146948.\n",
    "\n",
    "Max length of input documents ([caveat 1](https://github.com/tensorflow/hub/issues/244), [caveat 2](https://www.sbert.net/examples/applications/computing-embeddings/README.html?highlight=max#input-sequence-length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570d188-7d82-4ed5-b597-1f22107541b2",
   "metadata": {},
   "source": [
    "## With [Word Mover Distance](http://proceedings.mlr.press/v37/kusnerb15.pdf)\n",
    "\n",
    "[Gensim tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e215ef0f-70e7-473d-a2f1-73e5334a5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "13b12851-dfad-4f69-acb3-03503ac1c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_obama = 'Obama speaks to the media in Illinois'\n",
    "sentence_president = 'The president greets the press in Chicago'\n",
    "sentence_else= 'tomorrow is a rainy day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e3c4964f-d62b-41c5-956d-986e363f5b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4437976459291981"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.wmdistance(sentence_obama, sentence_president)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4764a7b8-037d-4790-8882-693d7f44424f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5462512284310116"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.wmdistance(sentence_obama, sentence_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "215433d6-3b65-432f-b350-5d7b0938c6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6227118478717114"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.wmdistance(sentence_president, sentence_else)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a11af-bd21-48ab-948c-d85bfa8169a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## With [universal-sentence-encoder](https://tfhub.dev/google/collections/universal-sentence-encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc2e4a44-7064-442a-a38d-fd8b7ee34843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 04:48:47.218741: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-02-13 04:48:47.218807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (csss-1): /proc/driver/nvidia/version does not exist\n",
      "2023-02-13 04:48:47.220014: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4df687f7-022c-4d41-8493-bbbc63f03846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13056417, -0.07187397,  0.0137589 , ...,  0.04946987,\n",
       "         0.02765515,  0.04919228],\n",
       "       [ 0.05786867, -0.06231322,  0.07939319, ...,  0.01655525,\n",
       "         0.00354518,  0.03140444],\n",
       "       [ 0.09586518, -0.0639194 , -0.03518083, ..., -0.01046547,\n",
       "         0.05872947,  0.00258046],\n",
       "       [ 0.05267328, -0.00014825, -0.04500623, ..., -0.07624684,\n",
       "        -0.00496952, -0.05112167],\n",
       "       [ 0.09143507, -0.03138757,  0.02067843, ..., -0.05256198,\n",
       "         0.06383334, -0.0003199 ]], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs1=['高兴', '狗狗真可爱', '出去走走', 'pets are our good friends', 'happy learning']\n",
    "docs1_vectors=embed(docs1).numpy()\n",
    "docs1_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "60602139-ad72-4f29-88e4-8d254228839a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04859294, -0.02410855,  0.01296188, ..., -0.06364327,\n",
       "        -0.01872791, -0.02179378],\n",
       "       [ 0.10388371, -0.04205937, -0.02334163, ..., -0.04688105,\n",
       "         0.04073469, -0.01615168]], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs2=['动物是我们的好朋友', '开心学习']\n",
    "docs2_vectors=embed(docs2).numpy()\n",
    "docs2_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "db913d4b-0ef9-4bb2-baff-d99289dc68fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16803929, 0.3675997 ],\n",
       "       [0.49006218, 0.21030766],\n",
       "       [0.00966788, 0.29758108],\n",
       "       [0.78957915, 0.11001315],\n",
       "       [0.1702948 , 0.76996684]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(docs1_vectors, docs2_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4b06c4a5-38c8-4158-8f3c-42b48ff414e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16803931, 0.36759971],\n",
       "       [0.49006217, 0.21030767],\n",
       "       [0.00966789, 0.29758106],\n",
       "       [0.78957921, 0.11001316],\n",
       "       [0.17029482, 0.76996691]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify: np.inner == 1-cosine distance\n",
    "from scipy import spatial\n",
    "1-spatial.distance.cdist(docs1_vectors, docs2_vectors, metric='cosine')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
